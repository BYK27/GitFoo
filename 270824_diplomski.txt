import tensorflow as tf
from keras.applications.inception_v3 import InceptionV3
from keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Activation
from keras.models import Model
from keras.optimizers import SGD, RMSprop
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, CSVLogger
import pandas as pd
import os

# Load and filter the verified train and val datasets
verified_train = pd.read_csv('meta/verified_train.tsv', sep='\t', header=None, names=['class_img_key', 'verification_label'])
verified_train = verified_train[verified_train['verification_label'] == 1]

verified_val = pd.read_csv('meta/verified_val.tsv', sep='\t', header=None, names=['class_img_key', 'verification_label'])
verified_val = verified_val[verified_val['verification_label'] == 1]

# Function to extract image paths and labels
def get_image_paths_and_labels(df, base_dir):
    image_paths = [os.path.join(base_dir, f'{row.split("/")[0]}/{row.split("/")[1]}.jpg') for row in df['class_img_key']]
    labels = [row.split('/')[0] for row in df['class_img_key']]
    return image_paths, labels

train_image_paths, train_labels = get_image_paths_and_labels(verified_train, 'images')
val_image_paths, val_labels = get_image_paths_and_labels(verified_val, 'images')

# Create ImageDataGenerator with strong augmentation
datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=45,
    width_shift_range=0.125,
    height_shift_range=0.125,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Function to create a generator
def create_generator(image_paths, labels, datagen, batch_size):
    df = pd.DataFrame({'filename': image_paths, 'class': labels})
    return datagen.flow_from_dataframe(df, x_col='filename', y_col='class', target_size=(299, 299), batch_size=batch_size, class_mode='categorical')

train_generator = create_generator(train_image_paths, train_labels, datagen, batch_size=32)
val_generator = create_generator(val_image_paths, val_labels, datagen, batch_size=32)

# Load pretrained InceptionV3 model
base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=(299, 299, 3)))

# Add custom layers on top
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(4096)(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(train_generator.num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

# Freeze all layers in the base model
for layer in base_model.layers:
    layer.trainable = False

# Compile the model
model.compile(optimizer=RMSprop(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks for saving the best model and logging
checkpointer = ModelCheckpoint(filepath='model_best.hdf5', verbose=1, save_best_only=True)
csv_logger = CSVLogger('training_log.csv')

# Train the model
model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10,
    callbacks=[csv_logger, checkpointer]
)

# Fine-tuning: Unfreeze some layers and recompile with a lower learning rate
for layer in base_model.layers[172:]:
    layer.trainable = True

model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=50,
    callbacks=[csv_logger, checkpointer]
)




(oldenv) PS D:\diplomski\Spas-Za-Has\src\model> python .\pretrain.py
Traceback (most recent call last):
  File ".\pretrain.py", line 15, in <module>
    verified_val = pd.read_csv('meta/verified_val.tsv', sep='\t', header=None, names=['class_img_key', 'verification_label'])
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\pandas\util\_decorators.py", line 311, in wrapper
    return func(*args, **kwargs)
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\pandas\io\parsers\readers.py", line 586, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\pandas\io\parsers\readers.py", line 482, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\pandas\io\parsers\readers.py", line 811, in __init__
    self._engine = self._make_engine(self.engine)
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\pandas\io\parsers\readers.py", line 1040, in _make_engine
    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\pandas\io\parsers\c_parser_wrapper.py", line 51, in __init__
    self._open_handles(src, kwds)
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\pandas\io\parsers\base_parser.py", line 229, in _open_handles
    errors=kwds.get("encoding_errors", "strict"),
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\pandas\io\common.py", line 707, in get_handle
    newline="",
FileNotFoundError: [Errno 2] No such file or directory: 'meta/verified_val.tsv'



(oldenv) PS D:\diplomski\Spas-Za-Has\src\model> python .\pretrain.py
Traceback (most recent call last):
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\pandas\core\indexes\base.py", line 3361, in get_loc
    return self._engine.get_loc(casted_key)
  File "pandas\_libs\index.pyx", line 76, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\index.pyx", line 108, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\hashtable_class_helper.pxi", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas\_libs\hashtable_class_helper.pxi", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'class'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ".\pretrain.py", line 42, in <module>
    train_generator = create_generator(train_image_paths, train_labels, datagen, batch_size=32)
  File ".\pretrain.py", line 40, in create_generator
    return datagen.flow_from_dataframe(df, x_col='filename', y_col='class', target_size=(299, 299), batch_size=batch_size, class_mode='categorical')
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\keras\preprocessing\image.py", line 1827, in flow_from_dataframe
    dtype=self.dtype,
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\keras\preprocessing\image.py", line 974, in __init__
    df, classes = self._filter_classes(df, y_col, classes)
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\keras\preprocessing\image.py", line 1111, in _filter_classes
    for v in df[y_col]:
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\pandas\core\frame.py", line 3458, in __getitem__
    indexer = self.columns.get_loc(key)
  File "D:\diplomski\Spas-Za-Has\oldenv\lib\site-packages\pandas\core\indexes\base.py", line 3363, in get_loc
    raise KeyError(key) from err
KeyError: 'class'












import pandas as pd
from keras.preprocessing.image import ImageDataGenerator

# Load the TSV file
df = pd.read_csv('meta/verified_train.tsv', sep='\t', header=0, names=['class_name/key', 'verification_label'])

# Split the class_name/key column
df[['class_name', 'filename']] = df['class_name/key'].str.split('/', 1, expand=True)

# Filter for verified images
df = df[df['verification_label'] == 1]

# Initialize ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)

# Create the data generator
train_generator = datagen.flow_from_dataframe(
    df,
    x_col='filename',
    y_col='class_name',
    directory='path_to_image_directory',  # adjust to your image directory path
    target_size=(299, 299),
    batch_size=32,
    class_mode='categorical'
)












import pandas as pd
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.inception_v3 import InceptionV3
from keras.layers import Input, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Dropout
from keras.models import Model
from keras.callbacks import ModelCheckpoint, CSVLogger
import keras.backend as K

# Load the TSV file
df = pd.read_csv('meta/verified_train.tsv', sep='\t', header=0, names=['class_name/key', 'verification_label'])

# Split the class_name/key column
df[['class_name', 'filename']] = df['class_name/key'].str.split('/', 1, expand=True)

# Filter for verified images
df = df[df['verification_label'] == 1]

# Set up ImageDataGenerator
datagen = ImageDataGenerator(
    featurewise_center=False,
    samplewise_center=False,
    featurewise_std_normalization=False,
    samplewise_std_normalization=False,
    zca_whitening=False,
    rotation_range=45,
    width_shift_range=0.125,
    height_shift_range=0.125,
    horizontal_flip=True,
    vertical_flip=False,
    rescale=1./255,
    fill_mode='nearest'
)

# Create the data generator
train_generator = datagen.flow_from_dataframe(
    df,
    x_col='filename',
    y_col='class_name',
    directory='path_to_image_directory',  # replace with the correct path to your images
    target_size=(299, 299),
    batch_size=32,
    class_mode='categorical'
)

# Initialize InceptionV3 model
K.clear_session()

base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=(299, 299, 3)))
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(4096)(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Dropout(.5)(x)
predictions = Dense(len(df['class_name'].unique()), activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

# Set up checkpoints and CSV logging
checkpointer = ModelCheckpoint(filepath='first.3.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)
csv_logger = CSVLogger('first.3.log')

# Train the model
model.fit(
    train_generator,
    validation_data=None,  # Replace this with your validation generator if available
    epochs=10,
    verbose=1,
    callbacks=[csv_logger, checkpointer]
)

# Fine-tuning
for layer in base_model.layers[:172]:
    layer.trainable = False
for layer in base_model.layers[172:]:
    layer.trainable = True

model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])

# Second round of training
checkpointer = ModelCheckpoint(filepath='second.3.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)
csv_logger = CSVLogger('second.3.log')

model.fit(
    train_generator,
    validation_data=None,  # Replace this with your validation generator if available
    epochs=100,
    verbose=1,
    callbacks=[csv_logger, checkpointer]
)

